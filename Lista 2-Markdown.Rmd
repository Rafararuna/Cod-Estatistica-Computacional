---
title: "Lista 2 - Otimização e Máxima Verossimilhança"
author: "Rafael Santana Araruna"
date: "16/02/2022"
output: pdf_document
---

\textbf{Exercício 1)} Seja $T$ uma variável aleatória seguindo uma distribuição Birnbaum-Saunders com parâmetros $\alpha > 0$ (forma) e $\beta > 0$ (escala), com notação $T \sim BS(\alpha ,\beta )$. 

\textbf{Problema:} Considere a seguinte amostra proveniente de uma $BS(\alpha = 0.5 ,\beta = 2.0 )$:

```{r message=FALSE, warning=FALSE, echo=TRUE}
dados <- c(2.8660833, 2.9974002, 3.7438357, 1.8547118, 1.8265233, 3.4525441,
       1.3621581, 3.8131878, 2.0969880, 2.7967733, 1.7850382, 1.5230772,
       1.3722042, 1.1855542, 3.1024554, 1.0848315, 2.2334891, 1.3668661,
       1.9122130, 1.7089584, 0.7841663, 1.9528258, 4.0061368, 2.0407105,
       0.8405855, 5.1794931, 1.6179411, 4.3789515, 1.4596143, 2.1202899)
```

Implemente dois códigos no R para estimar os parâmetros $\alpha$ e $\beta$ da BS através do método da máxima verossimilhança

* usando o método BFGS através da função optim;

* usando o método de Newton;

Como valores iniciais, pode-se usar $\hat{\alpha _{0}} = 0.1$ e $\hat{\beta _{0}} = 1.0$ .

\textbf{Resolução:} 

* Usando o método BFGS através da função optim:

Primeiramente, será definido as seguintes variáveis globais:

```{r message=FALSE, warning=FALSE, echo=F}
n <- length(dados) # tamanho da amostra
alpha <- 0.5 # valor verdadeiro de alpha
beta <- 2 # valor verdadeiro de beta

df <- data.frame("n" = c("30"), "alpha" = c(0.5), "beta" = c(2.0))

library(knitr)
knitr::kable(df,caption = "Variáveis Globais",align = "c")
```

Agora, a partir do seguinte código, realiza-se a implementação da função log-verossimilhança:

```{r message=FALSE, warning=FALSE, echo=T}
logL <- function(theta, x) {
  
  n     <- length(x)
  alpha <- theta[1]
  beta  <- theta[2]
  
  loglike <- n/(alpha^2) - 1/(2*alpha^2)*sum(x/beta+beta/x) -
    n*log(alpha) - (n/2)*log(beta) + sum(log(x+beta))
  
  return(-loglike)
}
```

Dessa forma, sabendo que os valores iniciais são $\hat{\alpha _{0}} = 0.1$ e $\hat{\beta _{0}} = 1.0$, implementa-se a função optim usando o método BFGS com o seguinte código:

```{r message=FALSE, warning=FALSE, echo=T}
## Valores iniciais:
alpha_0 <-  0.1
beta_0 <- 1.0

resulta <- optim(c(alpha_0, beta_0), logL, x = dados, method = "BFGS")
resulta$par
```

```{r message=FALSE, warning=FALSE, echo=F}
df <- data.frame( "alpha estimado" = resulta$par[1], "beta estimado" = resulta$par[2])

library(knitr)
knitr::kable(df,caption = "Parâmetros estimados",align = "c")
```

Analisando a tabela acima, nota-se que os valores das estimativas não estão perto dos verdadeiros valores dos parâmetros. Tal fato indica que possa ter um equívoco na escolha dos valores iniciais. Dessa forma, usando $\hat{\alpha _{0}} = 1.0$ e $\hat{\beta _{0}} = 1.0$, tem-se as seguintes estimativas:

```{r message=FALSE, warning=FALSE, echo=T}
## Valores iniciais:
alpha_0 <-  1.0
beta_0 <- 1.0

resulta <- optim(c(alpha_0, beta_0), logL, x = dados, method = "BFGS")
resulta$par
```

```{r message=FALSE, warning=FALSE, echo=F}
df <- data.frame("alpha estimado" = resulta$par[1], "beta estimado" = resulta$par[2])

library(knitr)
knitr::kable(df,caption = "Parâmetros estimados",align = "c")
```

Observando a tabela acima, nota-se que os valores estimados estão bastante próximos dos verdadeiros valores dos parâmetros. Assim, pode-se dizer que o erro anterior estava na escolha dos valores inicais, em específico, do valor de $\hat{\alpha _{0}}$, mostrando o quão relevante é decisão sobre qual valor inicial será utilizado na otimização.



* Usando o método de Newton:

Primeiramente, será definido as seguintes variáveis globais:

```{r message=FALSE, warning=FALSE, echo=F}
n <- length(dados) # tamanho da amostra
alpha <- 0.5 # valor verdadeiro de alpha
beta <- 2 # valor verdadeiro de beta

df <- data.frame("n" = c("30"), "alpha" = c(0.5), "beta" = c(2.0))

library(knitr)
knitr::kable(df,caption = "Variáveis Globais",align = "c")
``` 

Agora, sabendo que a questão nos forneceu a função de log-verossimilhança para $\theta = (\alpha ,\beta)^{T}$, as primeiras derivadas (gradientes) em relação à $\alpha$ e $\beta$ e a matriz Hessiana, podemos fazer a implementação do método de Newton-Raphson, a partir do seguinte código:

```{r message=FALSE, warning=FALSE, echo=T}
NR <- function(x, start, tol = 1e-10){
  
  theta <- start
  ch <- 1
  
  while(ch > tol){ 
    
    n <- length(dados)
    alpha <- theta[1]
    beta <- theta[2]
    
    l_alpha <- (-2*n/alpha^3) + ((1/alpha^3)*(sum((x/beta) + (beta/x)))) - n/alpha
    
    l_beta <- (1/(2*(alpha^2))*(sum((x/(beta^2))-(1/x)))) - (n/(2*beta)) + (sum(1/(x+beta)))
    
    gradiente <- c(l_alpha,l_beta)  
    
    l_alpha_alpha <- ((6*n)/(alpha^4)) - ((3/(alpha^4))*(sum((x/beta) + (beta/x)))) + (n/(alpha^2))
    
    l_alpha_beta <- (1/(alpha^3))*(sum((1/x) - (x/(beta^2))))
    
    l_beta_alpha <- (1/(alpha^3))*(sum((1/x) - (x/(beta^2))))
    
    l_beta_beta <- ((-1/((alpha^2)*(beta^3)))*(sum(x))) + (n/(2*(beta^2))) - (sum(1/((x+beta)^2))) 
    
    hessiana <- matrix(c( l_alpha_alpha,l_beta_alpha,l_alpha_beta,l_beta_beta), nrow = 2)
    
    theta <-  theta - gradiente%*%solve(hessiana)
    
    ch <- sqrt(t(gradiente)%*%gradiente)
    
  }
  
  return(theta)  
  
}
```

Dessa forma, sabendo que os valores iniciais são $\hat{\alpha _{0}} = 0.1$ e $\hat{\beta _{0}} = 1.0$, tem-se as seguintes estimativa:

```{r message=FALSE, warning=FALSE, echo=T}
## Valores iniciais:
v_iniciais <- c(0.1,1.0)

NR(dados, v_iniciais)
```

```{r message=FALSE, warning=FALSE, echo=F}
df <- data.frame( "alpha estimado" = 0.482957, "beta estimado" = 2.043902)

library(knitr)
knitr::kable(df,caption = "Parâmetros estimados",align = "c")
```
